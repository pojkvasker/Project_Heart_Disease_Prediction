package com.example.ppd_work;

public class NeuralNet {

    private enum Activation { IDENTITY, LOGISTIC, RELU, TANH, SOFTMAX }

    private Activation hidden;
    private Activation output;
    private double[][] network;
    private double[][][] weights;
    private double[][] bias;

    public NeuralNet(String hidden, String output) {

        // Parameters:
        int[] layers = {10, 10, 1};
        double[][][] weights = {{{-0.3120045458807879, 0.5104059265026887, -0.456844099102026, 0.5820043736263961, -0.2953620609734028, 0.5366522764406648, -0.0170925163663079, 0.23165763241161125, -0.19064783069268243, -0.04720137034996618}, {0.3677123903906327, -0.033327983878321865, 0.277907589527643, -0.3570286892749808, -0.5865943114760472, -0.3869387141666599, 0.09110847175586519, -1.1857147951031127e-08, -0.12631440809954406, -4.4698748972875746e-11}, {0.5838331706822785, 0.31666351514001717, 0.029818359256354673, -0.05321906275669639, 0.47914718978709736, 0.3921284600220727, -0.014941156750615346, -0.01176400470117809, -0.14274887401750633, -0.09315992428584095}, {-0.24697467548479649, 0.07526002432124845, 0.47348541003368533, -0.062179176068300465, 0.09174588392867511, -0.06837127426197834, -0.09340222271749038, -0.28735830351045133, -0.004132296105731517, -0.018063029329772717}, {0.06886375172506874, 0.13486722029813422, -0.25536376391765697, -0.11539867144925138, -0.04760667209427193, -0.07322890649706539, -0.08438629487099855, -0.06573582380250065, 0.023108457132945858, 0.027086968740430102}}, {{2.3789675522397446e-11, 0.14451429502917168, 9.011453834527004e-05, -0.017668789325305394, 0.5193182791695578, 0.09531067210686509, -0.4025844915208232, -0.2002078835283598, 0.20216288350196548, -0.2788360428087396}, {0.1700206506419504, -0.33698614848250685, 0.0008624360183352923, 0.004068032832165127, 0.08504190805950844, 0.13791131166304432, -0.10301065092290329, -0.02389876751706839, 0.10687108411405455, 0.285719608370694}, {-2.8817622721445845e-11, -0.12539871336109723, -1.4692973665040172e-11, 0.024249349772055655, -0.20993833693956035, 0.006657124289801596, -0.4055204556818684, -0.1256052556420864, -8.326085507109112e-12, -0.4479799360113482}, {-0.14126303184305047, 0.00410613142966842, -0.16683694251003353, -0.008539034389431205, 0.10304777471699374, 0.1070352296938222, 0.2827341382934521, -0.006339529112041924, 0.06052057753632698, -0.30941783886654295}, {-0.00033018149197681977, 0.1788127919419165, -0.2402376255790986, -0.0619576058988219, -0.4696830613826719, -0.42001548140015066, 0.5276748466902837, -0.09370671397150906, -0.17966581570217363, -0.03630890681486577}, {-0.2514334497798708, 0.20916414928023333, -0.05441251249500011, -2.399371146162344e-06, 0.40418022165020034, 0.47807924835735255, 0.28997645985558146, 0.035574590642502756, -0.21094956099939619, 0.050847818001509885}, {-0.0002236527626485149, 1.7843544591575545e-05, 0.1954390865401263, 0.17276537124183272, 4.2093955761352095e-11, -0.1573067848684024, 0.23931985389313856, -0.06917808104183311, 0.23114294872187827, 0.088268409702838}, {0.20761808704484713, 0.23929799338258811, -0.24331973710476745, -0.027365793425330538, 0.0016464919599797623, -0.07281504787306152, -0.03623015847147539, 1.7007585921968595e-05, 0.02502878209033651, -0.04529525246725321}, {0.09612955615326141, -0.21165849062029615, 0.06230164136985656, 0.03487700377143055, 0.0077823654683167615, -0.0002061355516972077, -4.014902309903216e-06, 0.16139389664832848, -0.0775779483272668, 8.238194697953016e-06}, {-0.07310315927263474, -0.13172388402418486, 0.18430265695293044, 0.00028468500876410925, 0.010073397256967268, 1.1964749339521647e-11, -0.19894615730491805, 0.003088808355392639, -0.17558626689035975, 0.16758170724557547}}, {{0.11715600153419888}, {-0.2877084981727256}, {0.31176728753239236}, {-0.11696252644635148}, {0.24442202886452818}, {0.354391921344129}, {-0.642252662377392}, {0.2935802114288527}, {-0.40443453563966014}, {0.16397492543086548}}};
        double[][] bias = {{0.08600704131826148, -0.3820606189900306, 0.009435681834147813, -0.5453047444110066, 0.24078347596315772, 0.3979981503287918, -0.539822677432387, -0.4302666709195392, -0.3959909716794696, -0.2525608017751087}, {-0.00417551298635388, 0.29896906379051574, -0.12892528666774417, 0.5320512162835662, 0.5499873819415648, -0.4948195490370127, 0.29625990368561955, -0.4891184672561468, -0.0634449392999421, -0.22455666230870056}, {-0.3102560249553799}};


        this.hidden = Activation.valueOf(hidden.toUpperCase());
        this.output = Activation.valueOf(output.toUpperCase());
        this.network = new double[layers.length + 1][];
        for (int i = 0, l = layers.length; i < l; i++) {
            this.network[i + 1] = new double[layers[i]];
        }
        this.weights = weights;
        this.bias = bias;
    }

    private double[] compute(Activation activation, double[] v) {
        switch (activation) {
            case LOGISTIC:
                for (int i = 0, l = v.length; i < l; i++) {
                    v[i] = 1. / (1. + Math.exp(-v[i]));
                }
                break;
            case RELU:
                for (int i = 0, l = v.length; i < l; i++) {
                    v[i] = Math.max(0, v[i]);
                }
                break;
            case TANH:
                for (int i = 0, l = v.length; i < l; i++) {
                    v[i] = Math.tanh(v[i]);
                }
                break;
            case SOFTMAX:
                double max = Double.NEGATIVE_INFINITY;
                for (double x : v) {
                    if (x > max) {
                        max = x;
                    }
                }
                for (int i = 0, l = v.length; i < l; i++) {
                    v[i] = Math.exp(v[i] - max);
                }
                double sum = 0.;
                for (double x : v) {
                    sum += x;
                }
                for (int i = 0, l = v.length; i < l; i++) {
                    v[i] /= sum;
                }
                break;
        }
        return v;
    }

    public int predict(double[] neurons) {
        this.network[0] = neurons;

        for (int i = 0; i < this.network.length - 1; i++) {
            for (int j = 0; j < this.network[i + 1].length; j++) {
                this.network[i + 1][j] = this.bias[i][j];
                for (int l = 0; l < this.network[i].length; l++) {
                    this.network[i + 1][j] += this.network[i][l] * this.weights[i][l][j];
                }
            }
            if ((i + 1) < (this.network.length - 1)) {
                this.network[i + 1] = this.compute(this.hidden, this.network[i + 1]);
            }
        }
        this.network[this.network.length - 1] = this.compute(this.output, this.network[this.network.length - 1]);

        if (this.network[this.network.length - 1].length == 1) {
            if (this.network[this.network.length - 1][0] > .5) {
                return 1;
            }
            return 0;
        } else {
            int classIdx = 0;
            for (int i = 0; i < this.network[this.network.length - 1].length; i++) {
                classIdx = this.network[this.network.length - 1][i] > this.network[this.network.length - 1][classIdx] ? i : classIdx;
            }
            return classIdx;
        }

    }
}
